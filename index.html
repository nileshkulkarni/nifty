<head>
    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript">
    google.load("jquery", "1.3.2");
    </script>    
</head>

<style type="text/css">
body {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
}

h1 {
    font-weight: 300;
    margin: 0.4em;
}

/* p {
    margin: 0.2em;
} */

.disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
    padding: 20px;
}

video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
}

img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
}

img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
}

a:link,
a:visited {
    color: #1367a7;
    text-decoration: none;
}

a:hover {
    color: #208799;
}

td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
}

.layered-paper-big {
    /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35),
        /* The third layer shadow */
        15px 15px 0 0px #fff,
        /* The fourth layer */
        15px 15px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fourth layer shadow */
        20px 20px 0 0px #fff,
        /* The fifth layer */
        20px 20px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fifth layer shadow */
        25px 25px 0 0px #fff,
        /* The fifth layer */
        25px 25px 1px 1px rgba(0, 0, 0, 0.35);
    /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
}


.layered-paper {
    /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35);
    /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
}

.vert-cent {
    position: relative;
    top: 50%;
    transform: translateY(-50%);
}

hr {
    margin: 0;
    border: 0;
    height: 1.5px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
}

.rotate {
    /* FF3.5+ */
    -moz-transform: rotate(-90.0deg);
    /* Opera 10.5 */
    -o-transform: rotate(-90.0deg);
    /* Saf3.1+, Chrome */
    -webkit-transform: rotate(-90.0deg);
    /* IE6,IE7 */
    filter: progid: DXImageTransform.Microsoft.BasicImage(rotation=0.083);
    /* IE8 */
    -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0.083)";
    /* Standard */
    transform: rotate(-90.0deg);
}

c {
    white-space: nowrap;
    writing-mode: tb-rl;
    transform: rotate(-180.0deg);
}

    .topnav {
      background-color: #eeeeee;
      overflow: hidden;
    }

    .topnav div {
      max-width: 1070px;
      margin: 0 auto;
    }

    .topnav a {
      display: inline-block;
      color: black;
      text-align: center;
      vertical-align: middle;
      padding: 16px 16px;
      text-decoration: none;
      font-size: 16px;
    }

    .topnav img {
      width: 100%;
      margin: 0.2em 0px 0.3em 0px;
    }
    .authors div{
        text-align: center;
    }
    .content{
        margin-bottom: 2em;
        font-size: 12pt;
    }
    p {
        display: block;
        margin-block-start: 1em;
        margin-block-end: 1em;
        margin-inline-start: 0px;
        margin-inline-end: 0px;
    }


</style>
<html>

<head>
    <title>NIFTY</title>
    <meta property="og:title" content="nlos" />
</head>

<body>	

    <br>
    <center>
    <p>
        <span style="font-size:42px"> NIFTY: Neural Object Interaction Fields <br>
            for Guided Human Motion Synthesis </span>
    </p>
    </center>
    <br>
    <div  align=center class="authors">
       <a href="https://nileshkulkarni.github.io/"> Nilesh Kulkarni<sup>1,2</sup></a> 
       &nbsp;  
       <a href="https://davrempe.github.io/"> Davis Rempe<sup>3</sup></a> 
       &nbsp;  
       <a href="https://www.kylegenova.com/"> Kyle Genova<sup>2</sup></a> 
       &nbsp;  
       <a href="https://abhijitkundu.info/"> Abhijit Kundu<sup>2</sup></a> 
       <br>
       <a href="https://web.eecs.umich.edu/~justincj/"> Justin Johnson<sup>1</sup></a> 
       &nbsp;  
       <a href="https://web.eecs.umich.edu/~fouhey/#"> David Fouhey<sup>1</sup></a> 
       &nbsp;  
       <a href="https://geometry.stanford.edu/member/guibas/"> Leonidas Guibas<sup>2,4</sup></a> 
      
    </div>
    <br>
    <!-- <div align=center class="authors">
        University of Michigan<sup>1</sup> 
        Google<sup>2</sup> 
        NVIDIA<sup>3</sup>
         

    </div> -->
    <div> 
        <table align=center width=500px>
            <tr>
                <td>
                    University of Michigan<sup>1</sup> 
                </td>
                <td>
                    Google<sup>2</sup> 
                </td>
                <td>
                    &nbsp;   &nbsp;  NVIDIA<sup>3</sup> 
                </td>
                <td>
                    Stanford University<sup>4</sup> 
                </td>
            </tr>
        </table>
       
    </div>
    <div align=center>
        Arxiv, 2023
    </div>
    
    <br>
    <table align=center width=400px>
        <tr>
            <td align=center width=50px>
                <center>
                    <a href="./assets/paper.pdf">[Paper]</a>
                </center>
            </td>
            <td align=center width=50px>
                <center>
                    <span > <a href="./supplementary.html">[Supp.] </a> </span> 
                </center>
            </td>
            <td align=center width=50px>
                <center>
                    <span > <a href="https://github.com/nileshkulkarni/nifty">[GitHub] </a> </span> 
                </center>
            </td>
        </tr>
    </table>
    <br>
    <br>
    <!-- <hr> -->
    <table align=center width=800px>
    <tr><td>
 
    <div  align=center class="content" width=400px>
    <img height=300 src="assets/fig1.png"> </img>
    <br>
    <p align="justify">
        We address the problem of generating realistic 3D motions of humans interacting
        with objects in a scene. Our key idea is to create a neural interaction field attached
        to a specific object, which outputs the distance to the valid interaction manifold
        given a human pose as input. This interaction field guides the sampling of an object-
        conditioned human motion diffusion model, so as to encourage plausible contacts
        and affordance semantics. To support interactions with scarcely available data,
        we propose an automated synthetic data pipeline. For this, we seed a pre-trained
        motion model, which has priors for the basics of human movement, with interaction-
        specific anchor poses extracted from limited motion capture data. Using our
        guided diffusion model trained on generated synthetic data, we synthesize realistic
        motions for sitting and lifting with several objects, outperforming alternative
        approaches in terms of motion quality and successful action completion. We call
        our framework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.
    </p>
    </div>
    </td>  </tr>
    </table>
    
    <center>
        <h1>Motion generation with Interaction Field Guidance</h1>
    </center>  
    <hr>
    <table  align=center width=800px>
        <tr>
        <td>
            <img align="center" width="800px" src="assets/model.png"></img>
        </td>
        </tr>
        <tr>
        <td>
            <p>
            Our motion synthesis model consists of an Object Interaction Field which guides the ouputs from 
            the diffusion model during sampling. The diffusion model is conditioned on the initial human pose, object
            geometry and object position. The object interaction field takes as input the last pose of the generated motion,
            and uses guidance to push the pose towards the valid interaction manifold.
            </p>
        </td>
        </tr>

    </table>
    <center>
        <h1>Synthetic Data Generation</h1>
    </center>
    <table align="center" width=800px>
        <tr>
        <td colspan='2'>
            <center>
            <video width="400" controls muted autoplay loop>
                <source src="./assets/TreeGens/sit_chairwood_all/Date05_Sub06_chairwood_sit_s101_e100_b100.mp4" type="video/mp4">
            </video>
            </center>
        </td>
        <td>
            <center>
                <video width="400"  controls muted autoplay loop>
                <source src="./assets/TreeGens/sit_tablesquare/Date02_Sub02_tablesquare_sit_s30_e29_b124.mp4" type="video/mp4">
            </video>
            </center>
        </td> 
        </tr>
    </table>
    <table align="center" width=800px>
        <tr>
            <td>
                <p>
                Given a final interaction pose of a person sitting on a chair / table we use a pre-trained motion model to 
                predict the past motion. Our generation follows a tree-like branching strategy and allows us to scalably create
                more data for an given interaction.
                </p>
            </td>
        </tr>
    </table>

    <center>
        <h1>Motion Generation Results with NIFTY</h1>
    </center>  
    <hr>
    <table  align=center width=800px>
        <tr>
            
            <td colspan='2'>
                <center>
                <video height="400" controls muted autoplay loop>
                    <source src="./assets/nifty_additional/medium_sit_cw.mp4" type="video/mp4">
                </video>
                </center>
            </td>   
            <td colspan='2'>
                <center>
                <video height="400" controls muted autoplay loop>
                    <source src="./assets/nifty_additional/medium_lift_stool.mp4" type="video/mp4">
                </video>
                </center>
            </td>     
        </tr>
        <tr>
        <td colspan='2'>
            <p align="center">
            Sitting on a chair
            </p>
        </td>
        <td colspan='2'>
            <p  align="center">
            Lifting a stool
            </p>
        </td>
        </tr>
    </table>
    <table align=center width=800px>
        <tr>
            <td>
                <a href="./supplementary.html#qual_results"><p align="center"> Additional results visit  this site</p> </a>
            </td>
        </tr>
    </table>
    <center>
        <h1>Qualitative Comparisons</h1>
    </center>  
    <hr>

    <table align=center width=800px>
        <tr>
            <td>
                We conduct an user study to evaluate the quality of generated motions and compare to other baseline methods.
            </td>
        </tr>
        <tr>
            <td>
                <br>
            </td>
        </tr>
        <tr>
            <td align="center">
                <img width=400px src="assets/user_study.png"/>
            </td>
        </tr>
        <tr>
            <td>
            We compare against two baselines a) Conditional VAE, and b) Conditional MDM. NIFTY's outputs are consistently 
            preferred over outputs from these methods. It is interesting to note that as compared to the 
            sythetic training data NIFTY is equally preferred.
            </td>
        </tr>
    </table>
    <table>
        <tr>
            <td colspan='4'>
                <center>
                <video height="400" controls muted autoplay loop>
                    <source src="./assets/baselines/medium_sit_baselines.mp4" type="video/mp4">
                </video>
                </center>
            </td>   
        </tr>
        <tr>
            <td align="center">
                <b> Sitting Interactions</b>
            </td>
        </tr>
    </table>
    <br><br>
    <table>
        <tr>
            <td colspan='4'>
                <center>
                <video height="400" controls muted autoplay loop>
                    <source src="./assets/baselines/medium_lift_baselines.mp4" type="video/mp4">
                </video>
                </center>
            </td>   
        </tr>
        <tr>
            <td align="center">
                <b>Lifting Interactions</b>
            </td>
        </tr>
    </table>
    <center>
        <h1>Paper</h1>
    </center>
    <hr>
    <br>
    <table align="center" width=800px>
        <tr>
            <td colspan="2">
                <img class="layered-paper-big" width=150px src="assets/paper.png"/>
            </td>
            <td colspan="2">
                <b>NIFTY: Neural Object Interaction Fields<br> 
                for Guided Human Motion Synthesis</b> <br>  <br>
                Kulkarni. N, Rempe. D, Genova K., Kundu A., <br>  Johnson J., Fouhey D., Guibas. L
                <br> <br>
                Arxiv, 2023 <br> <br>
                <a href="./assets/paper.pdf">[Paper] </a>  <a href="./assets/bibtex.txt"> [BibTex] </a>


            </td>
        </tr>
    </table>
    <br>

    <center>
        <h1>Acknowledgments</h1>
    </center>
    <hr>
    <p>
        We express our gratitude to our colleagues for the fantastic project discussions and feedback provided at different stages. We have organized them by institution (in alphabetical order) .
        <ul> 
            <li><i>Google</i> : Matthew Brown, Frank Dellaert, Thomas A. Funkhouser, Varun Jampani</li>
            <li><i>Google (co-interns)</i> : Songyou Peng, Mikaela Uy, Guandao Yang, Xiaoshuai Zhang </li> 
            <li><i>University of Michigan </i> :  Mohamed El Banani, Ang Cao,  Karan Desai,  Richard Higgins,  Sarah Jabbour, Linyi Jin, Jeongsoo Park, Chris Rockwell, Dandan Shan</li>
        </ul>

        This work was partly done when NK was interning at Google Research. DR was supported by the NVIDIA Graduate Fellowship. This project page template is based on <a href="https://research.nvidia.com/labs/toronto-ai/trace-pace/"> this page</a>.
    </p>
    <hr>
    <br>
    <table align="center">
        <tr> <td><p>
            ...
        </p>
     </td></tr>
    </table>
    

</body>

</html>